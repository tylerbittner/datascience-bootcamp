{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: Supervised Learning with scikit-learn\n",
    "- [DataCamp course link](https://www.datacamp.com/courses/supervised-learning-with-scikit-learn/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-load modules used later\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Classification\n",
    "- [Slides](slides/Supervised Learning with scikit-learn/ch1_slides.pdf)\n",
    "\n",
    "\n",
    "- Models are implemented as Python classes and serve two functions:\n",
    "  1. Implement algos for learning & predicting (behavior)\n",
    "  1. Store information learned from training/fitting (data)\n",
    "- **K-nearest neighbors**\n",
    "  - larger k = smoother decision boundary = less complex model\n",
    "  - smaller k = more granular decision boundary = more complex model = more prone to overfitting\n",
    "- Key model performance measure: \n",
    "  - **accuracy** = # correct predictions / total # of data points\n",
    "- **Model complexity curves** are awesome!\n",
    "\n",
    "\n",
    "- **Train/test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Regression\n",
    "- [Slides](slides/Supervised Learning with scikit-learn/ch2_slides.pdf)\n",
    "\n",
    "\n",
    "- **Loss function** - Typically uses OLS (Ordinary Least Squares) method in linear regression models. Same as minimizing *RMSE (Root Mean Squared Error)* of the *predictions* on the test set.\n",
    "- Key model performance measure: \n",
    "  - **R-squared** = **R²** = amount of variance in target variable predicted from feature variables.\n",
    "  \n",
    "\n",
    "- **k-fold cross-validation**\n",
    "  >```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "reg = linear_model.LinearRegression()\n",
    "cv_results = cross_val_score(reg, X, y, cv=5)\n",
    "```\n",
    "\n",
    "### Regularization\n",
    "- Purpose is to penalize & discourage large coefficients (which can lead to overfitting)\n",
    "- Denoted as *\"alpha\"* or *\"lambda\"*\n",
    "- Common methods:\n",
    "  - `Ridge` - (L2) Typical first choice for regression models.\n",
    "  - `Lasso` - (L1) Useful for feature selection; it minimizes least-impactful features.\n",
    "  - `Elastic net` - The penalty term is a linear combination of the L1 and L2 penalties:\n",
    "  >\\begin{equation*}\n",
    "l1\\_ratio = {a\\ ∗\\ L1\\ +\\ b\\ ∗\\ L2}\n",
    "\\end{equation*}\n",
    "\n",
    "  \n",
    "### Logistic regression\n",
    "- Used for *binary* classification (NOT for continuous target variables despite its name)\n",
    "- Key model performance measure:\n",
    "  - **ROC curve** (Receiver Operating Characteristic curve)\n",
    "  - **AUC** (Area Under the [ROC] Curve) - The larger the better!\n",
    "- Regularization param for logreg is **C**, which controls the *inverse* of the regularization strength. A *large C* can lead to an *overfit* model, while a *small C* can lead to an *underfit* model.\n",
    "\n",
    "### Support Vector Machines (SVM)\n",
    "- A discriminative classifier formally defined by a separating hyperplane.\n",
    "- Hyperparameters:\n",
    "  - C - controls the regularization strength\n",
    "  - gamma - controls the kernel coefficient\n",
    "- **Support-vector clustering (SVC)**: A similar method that also builds on kernel functions but is appropriate for *unsupervised* learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Fine-tuning your model\n",
    "- [Slides](slides/Supervised Learning with scikit-learn/ch3_slides.pdf)\n",
    "\n",
    "\n",
    "### \"Confusion matrix\" (That's what that thing is called!)\n",
    "- Better model performance measure:\n",
    ">\\begin{equation*}\n",
    "\\mathbf{accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\end{equation*}\n",
    "\n",
    "- Other Confusion matrix metrics:\n",
    ">\\begin{equation*}\n",
    "\\mathbf{precision} = \\frac{TP}{TP + FP}\n",
    "\\end{equation*}\n",
    "  - Intuition: Very few false positives.  High *precision* means for example that very few real emails were predicted as spam.\n",
    "\n",
    "\n",
    "- \n",
    ">\\begin{equation*}\n",
    "\\mathbf{recall/sensitivity/hit\\ rate} = \\frac{TP}{TP + FN}\n",
    "\\end{equation*}\n",
    "  - Intuition: High *recall* means for example that a high percentage of spam emails were predicted correctly as spam.\n",
    "\n",
    "\n",
    "- \n",
    ">\\begin{equation*}\n",
    "\\mathbf{F1\\ score} = 2 * \\frac{precision * recall}{precision + recall}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://www.medcalc.org/manual/_help/images/roc_intro3.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ROC curve example\n",
    "Image(url=\"https://www.medcalc.org/manual/_help/images/roc_intro3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning\n",
    "- Hyperparameters cannot be explicitely learned by fitting the model.\n",
    "- Trial & error thru **grid search** is an effective method for finding the best hyperparams. The **randomized search** alternative saves computation time.\n",
    "- Essential to use *cross-validation* with a **hold-out set** here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# OR\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Preprocessing and pipelines\n",
    "\n",
    "### Encoding categorical values\n",
    "- **One hot encoding** - Uses **\"dummy variables\"** to map/encode categorical values to binary values.\n",
    "  - scikit-learn: `OneHotEncoder()` \n",
    "  - pandas: `get_dummies()`\n",
    "- **Label encoding** - Method mapping each category to its own numerical value.\n",
    "\n",
    "### Imputing missing data\n",
    "- sklearn's `SimpleImputer()` uses `np.mean()` as default method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values='NaN', strategy='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### The Pipeline design pattern\n",
    "\n",
    "- The **`Pipeline`** object takes sequential list of steps\n",
    "- Output of one step is input to next step\n",
    "- Each `step` is a *tuple* with two elements:\n",
    "  - Name - string\n",
    "  - Transform - object implementing .fit() and .transform()\n",
    "- A Pipeline can itself also serve as a step\n",
    "- `make_pipeline(*steps)` is a shorthand constructor which does not require step names.\n",
    "\n",
    "\n",
    "- Preprocessing multiple dtypes -- Use sklearn helper functions to combine steps which can't be directly fed from one to another:\n",
    "  - [**`FunctionTransformer()`**](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) - Turns a Python function into a Pipeline-compatible Transformer object.  In our use case we created FunctionTransformer() objects to return numeric and text columns from DataFrame separately.\n",
    "  - [**`FeatureUnion()`**](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html) - Combines multiple feature matrixes from sub-pipelines into one 'union' step in pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "steps = [('imputation', imp),\n",
    "         ('logistic_regression', logreg)]\n",
    "\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters can be specified with the **'`step_name__parameter_name`'** notation for scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'SVM__C': [1, 10, 100],\n",
    "              'SVM__gamma': [0.1, 0.01]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing (aka Centering & Scaling)\n",
    "\n",
    "Typical normalization methods:\n",
    "- *Standardization*: All features centered around zero and variance of one.  Range from -1 to 1\n",
    "  - (Subtract the mean and divide by variance)\n",
    "- Range from 0 to 1\n",
    "  - (Subtract the minimum and divide by the range)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\n",
    "# Course: Machine Learning with the Experts: School Budgets\n",
    "- [DataCamp.com course link](https://www.datacamp.com/courses/machine-learning-with-the-experts-school-budgets/)\n",
    "- Full code used in this course is in [this Jupyter Notebook](https://github.com/datacamp/course-resources-ml-with-experts-budgets/blob/master/notebooks/1.0-full-model.ipynb)\n",
    "\n",
    "\n",
    "## Chapter 1: Exploring the raw data\n",
    "- [Slides](slides/Machine Learning with the Experts - School Budgets/ch1_slides.pdf)\n",
    "\n",
    "- **Log loss** for binary classification\n",
    "  \\begin{equation*}\n",
    "logloss = - \\frac{1}{N} \\sum_{i=1}^N (y_i log(p_i) + (1 - y_i)log(1 - p_i))\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://wiki.fast.ai/images/4/43/Log_loss_graph.png\" width=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"http://wiki.fast.ai/images/4/43/Log_loss_graph.png\", width=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  **[`.predict()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict) vs [`.predict_proba()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba)**: Output from the `LogisticRegression` model method `.predict()` is just binary 0/1 instead of probabilities ranging from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Creating a simple first model\n",
    "- [Slides](slides/Machine Learning with the Experts - School Budgets/ch2_slides.pdf)\n",
    "\n",
    "### Multi-class classification tips\n",
    "- This data has rarely-occuring labels, therefore a standard train-test-split may be error-prone.  \n",
    "  - Scikit-learn's **`StratifiedShuffleSplit`** is a solution, however it only works only with a *single target variable*.\n",
    "  - A helper method has been created to do multi-class stratified train-test split: [**`multilabel_train_test_split()`**](https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/data/multilabel.py)\n",
    "- **OneVsRestClassifier** fits a separate classifier for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP tools & tips\n",
    "\n",
    "- **`CountVectorizer()`** - Given an array of words this tokenizes, creates vocabulary, and calculates frequency counts. \"bag-of-words\" (I did this shit 18 years ago with Perl.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Improving your model\n",
    "- [Slides](slides/Machine Learning with the Experts - School Budgets/ch3_slides.pdf)\n",
    "\n",
    "- More Pipelines practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Learning from the experts\n",
    "- [Slides](slides/Machine Learning with the Experts - School Budgets/ch4_slides.pdf)\n",
    "\n",
    "A number of tricks (optimization methods) where used by the [DrivenData competition winner](https://www.drivendata.org/competitions/4/box-plots-for-education/).\n",
    "- NLP:\n",
    "  - n-gram usage\n",
    "  - Tokenization method\n",
    "  - Normalization (feature scaling)\n",
    "    - scikit-learn's [`MaxAbsScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html)\n",
    "  - [**Dimensionality reduction**](https://en.wikipedia.org/wiki/Dimensionality_reduction)\n",
    "    - scikit-learn's [`SelectKBest`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html), which uses a [chi-squared test](https://en.wikipedia.org/wiki/Chi-squared_test)\n",
    "- Stats:\n",
    "  - **Interaction terms**: A statistical method that lets your model express what happens if two features appear together.\n",
    "    - Implemented in scikit-learn's [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
    "    - Course uses custom [`SparseInteractions`](https://github.com/drivendataorg/box-plots-sklearn/blob/master/src/features/SparseInteractions.py) to add support for sparse matrixes to save memory.\n",
    "    - Equation:\n",
    "  \\begin{equation*}\n",
    "\\beta_1 x_1 + \\beta_2 x_2 + \\beta_3(x_1 * x_2)\n",
    "\\end{equation*}\n",
    "- Computation:\n",
    " - \"**Hashing trick**\" to reduce memory consumption with negligible loss in accuracy.\n",
    "   - sklearn's [`HashingVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.html) (used in place of `CountVectorizer`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
