{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load libraries used in notebook\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [1, 2, 3]\n",
    "# Handy ways to copy a list:\n",
    "y = list(x)\n",
    "y = x[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Handy list methods:\n",
    "x.append(2)\n",
    "x.count(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Numpy arrays\n",
    "\n",
    "- `numpy.array()` are especially useful for *element-wise* calculations.\n",
    "- Slicing: `my_array[rows, columns]`\n",
    "- All elements must be same data type.\n",
    "- The `+` operator does an *element-wise sum* in numpy arrays instead of concatinating lists\n",
    "- `numpy.corrcoef()` docs: https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.corrcoef.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Creates a *boolean mask* array\n",
    "np_x = np.array(x)\n",
    "gt_1 = np_x > 1\n",
    "# Which can be used to slice another ndarray\n",
    "print(np_x[gt_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 2])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interesting *type coercion* behavior for np arrays: booleans are converted to 0/1 and \n",
    "# element-wise operations are applied as usual.\n",
    "np.array([True, 1, 2]) + np.array([3, 4, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1  1.1  2.1  3.1]\n",
      " [ 4.1  5.1  6.1  7.1]]\n",
      "[[  0.2   1.2   2.2]\n",
      " [  3.2   4.2   5.2]\n",
      " [  6.2   7.2   8.2]\n",
      " [  9.2  10.2  11.2]]\n"
     ]
    }
   ],
   "source": [
    "# Handy way to create matrixes!!\n",
    "A = np.arange(8).reshape(2, 4) + 0.1\n",
    "print(A)\n",
    "B = np.arange(12).reshape(4, 3) + 0.2\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  1.1,  2.1,  3.1,  0.1,  1.1,  2.1,  3.1],\n",
       "       [ 4.1,  5.1,  6.1,  7.1,  4.1,  5.1,  6.1,  7.1]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking horizontally:\n",
    "C = np.hstack([A, A])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  1.1,  2.1,  3.1],\n",
       "       [ 4.1,  5.1,  6.1,  7.1],\n",
       "       [ 0.1,  1.1,  2.1,  3.1],\n",
       "       [ 4.1,  5.1,  6.1,  7.1]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking VERTICALLY:\n",
    "D = np.vstack([A, A])\n",
    "D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1,  1.1,  2.1,  3.1,  0.1,  1.1,  2.1,  3.1],\n",
       "       [ 4.1,  5.1,  6.1,  7.1,  4.1,  5.1,  6.1,  7.1]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another way to stack horizontally (or vertically):\n",
    "E = np.concatenate([A, A], axis=1)\n",
    "E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## array boolean operators for numpy and pandas\n",
    "- `logical_and()`\n",
    "- `logical_or()`\n",
    "- `logical_not()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False], dtype=bool)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.logical_and(np_x > 2, np_x <= 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting from np.array\n",
    "np_x[np.logical_and(np_x > 2, np_x <= 3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## pandas\n",
    "\n",
    "\n",
    "### Index slicing methods:\n",
    "- Full documentation here: [Indexing and Selecting Data](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "- Brackets only (for convenience but *not recommended*).  Aka Python's `__getitem__`.\n",
    "  - `df[]`\n",
    "  - if *column label* is passed, returns *columns* as Series.\n",
    "  - if *index slice using colon* is passed, returns *rows* as DataFrame.\n",
    "- Accessors (*preferred* for consistency and internal optimization):\n",
    "  - `.loc['row_selector', 'col_selector']` - *label*-based\n",
    "    - Gotcha watch: Label *ranges* can be passed with ':' as well.  Careful not to let this get confusing.\n",
    "  - `.iloc[row_selector, col_selector]` - *position*-based\n",
    "  - `.ix[]` - combines label *and* index-based selection\n",
    "  - Selectors can be passed as *lists* as well.\n",
    "- As attributes:\n",
    "  - You may access an index on a Series or column on a DataFrame directly as an attribute:\n",
    "  - `series.AAPL`\n",
    "  - `df.price`\n",
    "- **Based on conditions**:\n",
    "  - Return as new DataFrame:\n",
    "    - `df[df['first_name'].notnull() & (df['nationality'] == \"USA\")]`\n",
    "  - Return as boolean mask (to pass in subsequent conditions):\n",
    "    - `american = df['nationality'] == \"USA\"`\n",
    "- Gotcha watch: When slicing:\n",
    "  - *One* pair of brackets returns a **Series** - `df1['selector']`\n",
    "  - *Two* pairs of brackets returns a **DataFrame** - `df1[['selector']]`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      area    capital       country  population\n",
      "BR   8.516   Brasilia        Brazil      200.40\n",
      "RU  17.100     Moscow        Russia      143.50\n",
      "IN   3.286  New Delhi         India     1252.00\n",
      "CH   9.597    Beijing         China     1357.00\n",
      "SA   1.221   Pretoria  South Africa       52.98\n"
     ]
    }
   ],
   "source": [
    "# Notes setup\n",
    "brics_dict = {\n",
    "    \"country\": [\"Brazil\", \"Russia\", \"India\", \"China\", \"South Africa\"],\n",
    "    \"capital\": [\"Brasilia\", \"Moscow\", \"New Delhi\", \"Beijing\", \"Pretoria\"],\n",
    "    \"area\": [8.516, 17.10, 3.286, 9.597, 1.221],\n",
    "    \"population\": [200.4, 143.5, 1252, 1357, 52.98]\n",
    "}\n",
    "\n",
    "brics = pd.DataFrame(brics_dict)\n",
    "brics.index = [\"BR\", \"RU\", \"IN\", \"CH\", \"SA\"]\n",
    "print(brics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR          Brazil\n",
      "RU          Russia\n",
      "IN           India\n",
      "CH           China\n",
      "SA    South Africa\n",
      "Name: country, dtype: object\n",
      "         country\n",
      "BR        Brazil\n",
      "RU        Russia\n",
      "IN         India\n",
      "CH         China\n",
      "SA  South Africa\n"
     ]
    }
   ],
   "source": [
    "# Select column as Series - Use ONE pair of brackets\n",
    "print(brics[\"country\"])\n",
    "# Select column as DataFrame - Use TWO pairs of brackets\n",
    "print(brics[[\"country\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      area    capital country  population\n",
      "RU  17.100     Moscow  Russia       143.5\n",
      "IN   3.286  New Delhi   India      1252.0\n"
     ]
    }
   ],
   "source": [
    "# Only ROW selection can be done w/ numeric indexing\n",
    "print(brics[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "area            17.1\n",
      "capital       Moscow\n",
      "country       Russia\n",
      "population     143.5\n",
      "Name: RU, dtype: object\n",
      "    area capital country  population\n",
      "RU  17.1  Moscow  Russia       143.5\n"
     ]
    }
   ],
   "source": [
    "# Using loc: Select rows by index name **as Series** - use one pair of brackets\n",
    "print(brics.loc[\"RU\"])\n",
    "# Using loc: Select rows by index name **as DataFrame** - use TWO pairs of brackets\n",
    "print(brics.loc[[\"RU\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      area    capital       country  population  name_length\n",
      "BR   8.516   Brasilia        Brazil      200.40            6\n",
      "RU  17.100     Moscow        Russia      143.50            6\n",
      "IN   3.286  New Delhi         India     1252.00            5\n",
      "CH   9.597    Beijing         China     1357.00            5\n",
      "SA   1.221   Pretoria  South Africa       52.98           12\n"
     ]
    }
   ],
   "source": [
    "# Add new calculated row with .apply()\n",
    "brics['name_length'] = brics['country'].apply(len)\n",
    "print(brics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating\n",
    "\n",
    "Dictionaries:\n",
    "- `for key, val in my_dict.items():`\n",
    "\n",
    "Numpy arrays:\n",
    "- `for val in np.nditer(np_x):`\n",
    "\n",
    "Dataframes:\n",
    "- `for label, row_series in df.iterrows():`\n",
    "\n",
    "enumerate()\n",
    "- **`enumerate()`** returns an enumerate object that produces a sequence of tuples which are **index-value pairs** (vs. not having an index available in *iterator* objects)\n",
    "  - `for index1, value1 in enumerate(mutants):`\n",
    "\n",
    "zip()\n",
    "- Combines any number of iterables into a zip object which is an iterator of tuples\n",
    "- Can be printed with a splat: \n",
    "  - `print(*zipped)`\n",
    "- You can \"unzip\" zipped list by splat-unpacking into positional args then zip'ing back up:\n",
    "  - `orig_list1, orig_list2 = zip(*zipped)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List comprehensions\n",
    "- Syntax:\n",
    "  - `[*output expression* for *iterator variable* in *iterable*]`\n",
    "- Conditionals can be included in **output expression** (to filter and modify output) or at **end** (to filter output):\n",
    "  - `[[*output expression* if *conditional on output*] for *iterator variable* in *iterable*]`\n",
    "  - `[*output expression* for *iterator variable* in *iterable* if *conditional on iterable*]`\n",
    "- Can be **nested** with this syntax ([output expression] is itself a L.C.):\n",
    "  - `[[*output expression*] for *iterator variable* in *iterable*]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4], [0, 1, 2, 3, 4]]\n"
     ]
    }
   ],
   "source": [
    "matrix = [[col for col in range(0, 5)] for row in range(0, 5)]\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generators\n",
    "Two ways to define generators:\n",
    "1. List comprehension-like syntax:\n",
    "   - Same syntax as list comprehensions except *parens ()* instead of brackets [].\n",
    "   - Like a list comprehension except puts the output in an iterable **generator** object instead of an in-memory list.  \"Lazy evaluation\".  Handy for very large data sets which don't easily fit in memory.\n",
    "2. Generator functions: \n",
    "   - produce an iterable **generator** object: `yield` instead of `return`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scopes\n",
    "- \"LEGB\" order rule: Local, Enclosing, Global, Built-ins\n",
    "\n",
    "- global: access vars outside functions\n",
    "- nonlocal: access vars in enclosing scope (e.g. inside a nested function)\n",
    "\n",
    "### Closure example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "125\n"
     ]
    }
   ],
   "source": [
    "def raise_val(n):\n",
    "    \"\"\"Return the inner function.\"\"\"\n",
    "    \n",
    "    def inner(x):\n",
    "        \"\"\"Raise x to the power of n.\"\"\"\n",
    "        raised = x ** n\n",
    "        return raised\n",
    "\n",
    "    return inner\n",
    "\n",
    "square = raise_val(2) \n",
    "cube = raise_val(3)\n",
    "\n",
    "print(square(4))\n",
    "print(cube(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function tips\n",
    "- `*args` gets turned into a tuple\n",
    "- `**kwargs` gets turned into a dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Importing data\n",
    "\n",
    "## Numpy import methods:\n",
    "- `np.loadtxt(filename, delimeter=',', skiprows=1, usecols=[0, 2], dtype=str)`\n",
    "- `np.genfromtxt(filename, delimiter=',', names=True, dtype=None)`\n",
    "- `np.recfromcsv()` -- same as genfromtxt() but with dtype=None by default\n",
    "\n",
    "## pandas import methods:\n",
    "- `pd.read_csv(filename)`\n",
    "- `dict = pd.read_excel(filename)`\n",
    "  - Note the output of pd.read_excel() is a Python *dictionary* with sheet names as keys and corresponding DataFrames as corresponding values.\n",
    "- `xl = pd.ExcelFile(filename)`\n",
    "  - Get sheet names: `xl.sheet_names`\n",
    "  - Then `df = data.parse(\"sheetname\")`\n",
    "  - Or by index `df = data.parse(0)`\n",
    "\n",
    "**pickled files:**\n",
    "- `open(\"filename.pkl\", 'rb')`\n",
    "\n",
    "**SAS files:**\n",
    "```\n",
    "with SAS7BDAT(filename) as file:\n",
    "    df_sas = file.to_data_frame()\n",
    "```\n",
    "**Stata files:**\n",
    "- `df = pd.read_stata(filename)`\n",
    "\n",
    "**HDF5 files:**\n",
    "- \"Hierarchical Data Format\" - Good for large amounts of *numerical* data\n",
    "- `import h5py`\n",
    "- `data = h5py.File(file, 'r')`\n",
    "- Print *groups* in the file:\n",
    "```\n",
    "for key in data.keys():\n",
    "    print(key)\n",
    "```\n",
    "- Then get value of data within groups:\n",
    "  - `strain = data['strain']['Strain'].value`\n",
    "  \n",
    "**Matlab files:**\n",
    "- `scipy.io.loadmat(filename)`\n",
    "- `scipy.io.savemat(filename)`\n",
    "\n",
    "**Relational databases:**\n",
    "- With SQLAlchemy:\n",
    "```\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///Northwind.sqlite')\n",
    "with engine.connect() as con:\n",
    "    rs = con.execute('select * from ...')\n",
    "    df = pd.DataFrame(rs.fetchall())\n",
    "    df.columns = rs.keys()\n",
    "```\n",
    "- Or super concise with pandas:\n",
    "  - `df = pd.read_sql_query(\"SELECT * FROM Orders\", engine)`\n",
    "  \n",
    "**From web directly:**\n",
    "- With pandas:\n",
    "  - `df = pd.read_csv(url, sep='\\t')`\n",
    "- With urllib:\n",
    "```\n",
    "from urllib.request import urlopen, Request\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()\n",
    "```\n",
    "- With Requests module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Response', 'Error'])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"http://www.datacamp.com/teach/documentation\"\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "#print(text)\n",
    "\n",
    "# JSON output:\n",
    "url = 'http://www.omdbapi.com/?apikey=ff21610b&t=social+network'\n",
    "r = requests.get(url)\n",
    "json_data = r.json()\n",
    "print(json_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup formatting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html_doc = text\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "pretty_soup = soup.prettify()\n",
    "html_text = soup.get_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Data Cleaning & Transformation with pandas\n",
    "(Need to increase **`pd.options.display.max_columns = 20`** for iPython in this DataCamp course for whatever reason.)\n",
    "\n",
    "\"A *matrix* has rows & columns.  A *DataFrame* has observations and variables.\" -- Hadley Wickham\n",
    "\n",
    "Exploratory data analysis techniques:\n",
    "- Frequency counts\n",
    "  - `df.info()`\n",
    "  - `value_counts(dropna=False)` - Also sorts by default, so good for rankings.\n",
    "  - `unique()`\n",
    "- Summary statistics\n",
    "  - `df.describe()`\n",
    "- Visualize the data:\n",
    "  - boxplot for summary stats\n",
    "  - bars for discrete\n",
    "  - histograms for continuous\n",
    "  - scatter for comparing numerical vars\n",
    "- Note: When applying `.apply()` over an entire DataFrame, and not just one column or row, you'll have to chain the `.all()` method *twice*.\n",
    "  \n",
    "## Tidy Data\n",
    "[Tidy Data paper](https://www.jstatsoft.org/article/view/v059i10/v59i10.pdf) by Hadley Wickham\n",
    "\n",
    "\"Tidy data\" is a standard way of mapping the meaning of a dataset to its structure.  In tidy data:\n",
    "1. Each variable (attribute) forms a column.\n",
    "2. Each observation forms a row.\n",
    "3. Each type of observational unit (entity) forms a table.\n",
    "(Same as Codd’s 3rd normal form!)\n",
    "\n",
    "Key definitions (framed in a language familiar to statisticians):\n",
    "- **data set**: a collection of values\n",
    "- **values** are organized in two ways:\n",
    "    - **variable**: contains all values measuring an *attribute* (like height, temp, duration) *across units*.\n",
    "    - **observation**: contains all values measured on the same *unit* (\"entity\" in relational DB terms) (like a person, a day, a race) *across attributes*. A measurement.\n",
    "\n",
    "## Key exploratory methods\n",
    "- head()\n",
    "- info()\n",
    "- columns\n",
    "- dtypes\n",
    "- describe()\n",
    "- df.column.value_counts()\n",
    "- df.column.plot('hist')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoting\n",
    "A pivot table allows you to see all of your variables as a function of two other variables; one indexed as rows and one indexed as columns.\n",
    "- Explained in other ways, pivoting:\n",
    "  - Turns rows of data into columns.\n",
    "  - Creates a new column for each unique value in a specified column.\n",
    "  - Turns data from an \"analysis-friendly\" shape into a \"reporting-friendly\" shape (which violates \"Tidy Data\" principle of rows containing observations)\n",
    "- The `index` param specifies columns NOT to pivot; to use as row indexes.\n",
    "- `margins=True` param adds a grand total row.\n",
    "- Use `df.reset_index()` to flatten the columns of the pivoted DataFrame\n",
    "\n",
    "**`pivot()` method**: -- \"Allows you to **transform or reshape** data in a DataFrame.\"\n",
    "```\n",
    "df_pivoted = df.pivot(index='date', columns='element', values='value') \n",
    "```\n",
    "**`pivot_table()` method** -- \"Used to **summarize and aggredate** data in a DataFrame.\" A generalization of `pivot()` that can handle duplicate values for one index/column pair (MultiIndex). `aggfunc` param can be used to remove duplicate values/rows for example:\n",
    "- `index` and `columns` params can be lists in this function\n",
    "```\n",
    "df_pivoted = df.pivot_table(index='date', columns='element', values='value', aggfunc=np.mean) \n",
    "```\n",
    "- `index=` can also be a `pd.Grouper` object for handy aggregation/transformation (e.g. grouping dates by month)\n",
    "\n",
    "\n",
    "## Melting\n",
    "- Turns set of columns of data into rows as a single column.  Explained in other ways, the goal of melting is to:\n",
    "  - Change a DataFrame from a wide shape to a long shape.\n",
    "  - Restore a *pivoted* DataFrame to its original form.\n",
    "- Important params:\n",
    "  - **id_vars** (*tuple, list, or ndarray; optional*): explicitly specifies *columns* that should remain in the reshaped DataFrame.\n",
    "  - **value_vars** (*tuple, list, or ndarray; optional*): columns to convert into values (\"unpivot\").\n",
    "  - **var_name**: Name to use for the ‘variable’ column. \n",
    "  - **value_name**: Name to use for the ‘value’ column.\n",
    "  - **col_level**: If you have a multi-level *column* index, specify which level of it to melt.\n",
    "  \n",
    "- Example:\n",
    ">```\n",
    "pd.melt(frame=df, id_vars='name', \n",
    "    value_vars=['treatment a', 'treatment b'], \n",
    "    var_name='treatment', value_name='result') \n",
    "```\n",
    "\n",
    "## Concatinating\n",
    "- `pd.concat([df1, df2, df3])`\n",
    "- `axis=0` by default\n",
    "\n",
    "## Merging\n",
    "- `pd.merge(left=df1, right=df2, on=common_key_name)`\n",
    "- If key names in DFs are different: `pd.merge(left=df1, right=df2, left_on='key1', right_on=key2)` \n",
    "\n",
    "## Data types\n",
    "- `df.dtypes` to see data types [note it's an *attribute*]\n",
    "- Converting types:\n",
    "  - `df['converted_col'] = df['source_col'].astype(str)`\n",
    "  - `df['converted_col'] = pd.to_numeric(df['source_col'], errors='coerce')`\n",
    "  \n",
    "## Missing data\n",
    "- `df1.drop_duplicates()`\n",
    "- `df1.dropna()`\n",
    "- `df1['sparce_column'].fillna()`\n",
    "- `df1['sparce_column'].fillna(mean_value)`\n",
    "- `ps.isnull(obj)` - Detect missing values for an array-like object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "- My jam.\n",
    "- In Python, you can compile the regex first for speed\n",
    "\n",
    "- Useful methods:\n",
    "```\n",
    "Series.str.contains(pattern)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_sre.SRE_Match'>\n",
      "True\n",
      "['5', '16']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "result = re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890')\n",
    "print(type(result))\n",
    "print(bool(result))\n",
    "print(re.findall(pattern='\\d+', string='5 strawberries and 16 bananas'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Course: pandas Foundations\n",
    "(Need to increase `pd.options.display.max_columns = 10` for iPython in this DataCamp course.)\n",
    "\n",
    "## pandas Data Structures:\n",
    "- An **Index** is a sequence of labels with.  Simple.\n",
    "  - Has a homogeneous data type\n",
    "  - Immutable (like dict keys)\n",
    "  - *name* attribute\n",
    "- A **Series** is a 1D array with an index.\n",
    "  - The *values* of a Series are of type `numpy.ndarray` and can be returned with `series.values`.\n",
    "- A **DataFrame** is a 2D labelled array whose columns are Series.  They share a common Index.\n",
    "  - A *column* in a DataFrame is a Series. \n",
    "  \n",
    "- The set of *columns* can also be named: `df.columns.name = 'Vegetables'`\n",
    "  - This is equivalent to naming a \"column index\" just like a regular row index can be named."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling DataFrames from scratch\n",
    "- Can be composed from a dictionary of key-value pairs (*keys* are column headers, *values* are rows).\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Country', ['United States', 'Soviet Union', 'United Kingdom']), ('Total', [1118, 473, 273])]\n",
      "          Country  Total\n",
      "0   United States   1118\n",
      "1    Soviet Union    473\n",
      "2  United Kingdom    273\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Prepare two lists\n",
    "list_keys = ('Country', 'Total')\n",
    "list_values = (['United States', 'Soviet Union', 'United Kingdom'], [1118, 473, 273])\n",
    "\n",
    "# Zip the 2 lists together into one list of (key,value) tuples\n",
    "zipped = list(zip(list_keys, list_values))\n",
    "print(zipped)\n",
    "\n",
    "# Build a dictionary with the zipped list\n",
    "data = dict(zipped)\n",
    "\n",
    "# Build and inspect a DataFrame from the dictionary\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "- Can be used when creating *new* DFs as well as modifying existing DFs\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               city state\n",
      "0           Manheim    PA\n",
      "1      Preston park    PA\n",
      "2       Biglerville    PA\n",
      "3           Indiana    PA\n",
      "4      Curwensville    PA\n",
      "5             Crown    PA\n",
      "6      Harveys lake    PA\n",
      "7   Mineral springs    PA\n",
      "8         Cassville    PA\n",
      "9        Hannastown    PA\n",
      "10        Saltsburg    PA\n",
      "11      Tunkhannock    PA\n",
      "12       Pittsburgh    PA\n",
      "13        Lemasters    PA\n",
      "14       Great bend    PA\n"
     ]
    }
   ],
   "source": [
    "cities = ['Manheim', 'Preston park', 'Biglerville', 'Indiana', 'Curwensville', \n",
    "          'Crown', 'Harveys lake', 'Mineral springs', 'Cassville', 'Hannastown', \n",
    "          'Saltsburg', 'Tunkhannock', 'Pittsburgh', 'Lemasters', 'Great bend']\n",
    "\n",
    "# Construct a dictionary: data\n",
    "data = {'state': 'PA', 'city': cities}\n",
    "\n",
    "# Construct a DataFrame from dictionary data: df\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful read_csv() params\n",
    "- **header**: None or integer for telling it how many rows to consider headers\n",
    "- **names**: column labels\n",
    "- **index_col**\n",
    "- **na_values**: values to consider as empty/NaN.  Can be a scalar, list, or dictionary (to specify which columns to apply this to).\n",
    "- **parse_dates**: list of lists with column positions to treat as dates.  Each list is combined into one date field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual EDA\n",
    "\n",
    "- Three diff plotting idioms with DataFrames (check documentation)!\n",
    "  1. `df.plot(kind='hist')`\n",
    "  1. `df.plot.hist()`\n",
    "  1. `df.hist()`\n",
    "- Useful *histogram* plot() options:\n",
    "  - **bins**: number of bins\n",
    "  - **range** (tuple): min and max (\"extrema\") of bins\n",
    "  - **normed** (boolean): whether to normalize to one (aka Probability Density Function (PDF))\n",
    "  - **cumulative** (boolean): whether to compute Cumulative Distribution Function (CDF). Requires normed=True as well.\n",
    "  \n",
    "## Key Statistical EDA Methods\n",
    "\n",
    "- count()\n",
    "- mean()\n",
    "- std()\n",
    "- median()\n",
    "- quantile(q) -- Aka \"percentiles\". Computes median by default.\n",
    "- min(), max()\n",
    "- unique() -- Distinct categories\n",
    "- nunique() -- *Number* of distinct categories\n",
    "- idxmin(), idxmax()\n",
    "\n",
    "Note all DF statistical methods *ignore null entries*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Time Series\n",
    "\n",
    "### Indexing topics:\n",
    "- An Index is a special type of Series.\n",
    "- DateTimeIndex\n",
    "- `df1.set_index('Date column name', inplace=True)`\n",
    "- `pd.to_datetime()`\n",
    "- Partial string indexing and slicing using datetimes.\n",
    "  - Example showing datetime slice and the columns to include: `df1.loc['2010-Aug', 'Temperature']`\n",
    "- Reindexing: `df1.reindex(index)`\n",
    "  - `method='ffill'`\n",
    "  - `method='bfill'`\n",
    "\n",
    "### Resampling (downsampling, upsampling)\n",
    "  - `df1.resample('freq string')`\n",
    "  - Examples: 'D', '2W', '5A'\n",
    "  - Common freqency strings (Full reference: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases):\n",
    "  ```\n",
    "‘min’,‘T’ minute\n",
    "‘H’       hour\n",
    "‘D’       day\n",
    "‘B’       business day\n",
    "‘W’       week\n",
    "‘M’       month\n",
    "‘Q’       quarter\n",
    "‘A’       year\n",
    "  ```\n",
    "  - `df1.rolling(window=24).mean()`: Rolling mean, aka moving average.  **window=** specifies number of samples to aggregate.\n",
    "\n",
    "### Datetime methods:\n",
    "- `df1['datetime_column'].dt.hour`\n",
    "- `df1['datetime_column'].dt.date`\n",
    "- `df1['datetime_column'].dt.tz_localize('US/Pacific')`\n",
    "- `df1['datetime_column'].dt.tz_convert('US/Central')`\n",
    "- etc...\n",
    "\n",
    "### Interpolation\n",
    "Example using census data which is gathered once every decade:\n",
    "- `population.resample('A').first().interpolate(how='linear')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "# Course: Manipulating DataFrames with pandas\n",
    "\n",
    "## Filtering w/ boolean masks\n",
    "\n",
    "Return a boolean mask:\n",
    "- `df.column <conditional expression>`\n",
    "\n",
    "Filtering zeros values:\n",
    "- `df.all()` - returns only columns with *all non-zero* values.  Aka excludes cols containing zero values. (Gotcha watch!)\n",
    "- `df.any()` - returns columns with *any non-zero* values.  Aka includes cols containing non-zero values. (Gotcha watch!)\n",
    "\n",
    "Filtering NaN values:\n",
    "- `df.isnull()`\n",
    "- `df.notnull()`\n",
    "- Can be chained with `.all()` and `.any()`\n",
    "- `df.dropna(how='any|all')`\n",
    "  - Other options: `thresh=<count of NaNs>`, `axis='columns|rows'`\n",
    "  \n",
    "## Transforming DataFrames\n",
    "\n",
    "Vectorized methods (work on DFs and Series):\n",
    "  - `df.floordiv(12)`\n",
    "  - Universal Functions (\"ufunc\")\n",
    "    - `np.floor_divide(df, 12)`\n",
    "    - Numpy's operate on ndarrays in an element-by-element fashion.\n",
    "  - Handy accessor: `str`\n",
    "    - `df.str.lower()`\n",
    "  - Arithmatic operators work element-wise too\n",
    "\n",
    "Slower techniques (use Python for-loops internally):\n",
    "- `df.apply(function)`\n",
    "- `df.apply(lambda x: x//12)`\n",
    "- An index is a special kind of Series\n",
    "  - `.apply()` does NOT work on indexes -- use `map()` instead.\n",
    "  - `df.index.map(str.lower)`\n",
    "  \n",
    "Delete a column with:\n",
    "- `del df['col_name']`\n",
    "\n",
    "## Hierarchical Indexes\n",
    "Full documentation: [MultiIndex / Advanced Indexing](http://pandas.pydata.org/pandas-docs/stable/advanced.html)\n",
    "\n",
    "- A **MultiIndex** contains multiple, possibly hierarchical column names. Composed as a tuple of column names.\n",
    "  - `stocks = stocks.set_index(['Symbol', 'Date'])`\n",
    "  - [Gotcha watch:] `df.set_index()` *returns a new DataFrame* by default.\n",
    "- Sort a MultiIndex to enable slicing by range:\n",
    "  - `df.sort_index()`\n",
    "- \"Fancy\" indexing is passing slice ranges as a *list*:\n",
    "  - `stocks.loc[(['AAPL', 'MSFT'], '2016-10-05'), :]`\n",
    "  - `stocks.loc[('CSCO', ['2016-10-05', '2016-10-03']), :] `\n",
    "- [Gotcha watch:] Use `slice()` to force support for *both rows and columns* when needed (because the colon between ranges is not natively supported with tuples):\n",
    "  - `stocks.loc[(slice(None), slice('2016-10-03', '2016-10-04')), :]`\n",
    "  - `stocks.loc[(slice(None), '2016-10-05')), :]`\n",
    "  \n",
    "## Stacking & unstacking DataFrames\n",
    "Allows you fine control of pivoted DF where a MultiIndex is already present.\n",
    "\n",
    "Unstack a key in a MultiIndex (moves a key from the index to a column):\n",
    "- `trials.unstack(level='gender')` or `trials.unstack(level=1)`\n",
    "- Note this results in hierarchical columns as *tuples*.\n",
    "\n",
    "Stack a key in a MultiIndex (moves a column to the index):\n",
    "- `trials.stack(level='gender')`\n",
    "\n",
    "Swap order of keys with:\n",
    "- `df_multiindexed.swaplevel(0, 1)`\n",
    "- Then re-sort with `df_multiindexed.sort_index()`\n",
    "\n",
    "## Groupby and Categoricals\n",
    "- `df.groupby('column').count()` -- split into groups of rows by distinct values of given column then apply count() aggregation function to combine each group.\n",
    "  - \"split-apply-combine\" paradigm\n",
    "  - `groupby()` returns a `pandas.core.groupby.DataFrameGroupBy` object.\n",
    "  - `DataFrameGroupBy.groups` is a *dictionary* (which can be handy to see the split groups).\n",
    "- Can also choose *columns to apply the aggretation to*:\n",
    "  - `df.groupby('column')['apply_col'].count()`\n",
    "  - `sales.groupby('city')[['bread', 'butter']].sum()`\n",
    "\n",
    "with `MultiIndex`:\n",
    "- `gapminder.groupby(level=['Year', 'region'])`\n",
    "  \n",
    "The **`category`** data type can be handy; it reduces memory needed, and speeds up operations like `groupby()`:\n",
    "- `df['column'] = df['column'].astype('category')` \n",
    "\n",
    "Multiple aggregations at once:\n",
    "- `.agg(['sum', 'max', 'count', 'mean'])`\n",
    "- `.agg(custom_function)`\n",
    "\n",
    "You can also pass *dictionaries* to `agg()` to apply aggregations per column:\n",
    "- `.agg({'bread':'sum', 'butter':custom_function})`\n",
    "  \n",
    "An **aggregation** (`.agg()`) does a *reduction*, while a **transformation** (`.transform()`) *applies a function element-wise to a sequence*.\n",
    "\n",
    "An **`apply()`** is used for more complex transforms which don't fit into either of these paradigms.\n",
    "\n",
    "Using **z-scores** is a great way to identify outliers:\n",
    "- `standardized = gapminder_2010.groupby('region')['life','fertility'].transform(zscore)`\n",
    "\n",
    "## Groupby and Filtering\n",
    "Filter a `groupby()` group using *iteration*:\n",
    "```\n",
    "for group_name, group in splitting: \n",
    "    chevy_mean = group.loc[group['name'].str.contains('chevrolet'), 'mpg'].mean()\n",
    "    chevy_means[group_name] = avg\n",
    "```\n",
    "\n",
    "Or by using a *dictionary comprehension*:\n",
    "```\n",
    "chevy_means = {year:group.loc[group['name'].str.contains('chevrolet'), 'mpg'].mean(): for year, group in splitting}\n",
    "```\n",
    "\n",
    "And you can do a comparison between all and a filtered group using a boolean mask:\n",
    "```\n",
    "chevy = auto['name'].str.contains('chevrolet')\n",
    "auto.groupby(['yr', chevy])['mpg'].mean() \n",
    "```\n",
    "\n",
    "`DataFrame.filter()` - Subset rows or columns of a dataframe according to *labels* in the specified index. Does NOT filter by *content*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Course: Merging DataFrames with pandas\n",
    "[Course link](https://www.datacamp.com/courses/merging-dataframes-with-pandas)\n",
    "\n",
    "## Which methods to use when?\n",
    "- **`df1.append(df2)`**: stacking vertically\n",
    "- **`pd.concat([df1, df2])`**\n",
    "  - stacking many horizontally or vertically \n",
    "  - simple inner/outer joins on Indexes\n",
    "- **`df1.join(df2)`**: inner/outer/le!/right joins on Indexes \n",
    "- **`pd.merge([df1, df2])`**: many joins on multiple columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Preparing data\n",
    "[Slides](slides/Merging DataFrames with pandas/ch1_slides.pdf)\n",
    "\n",
    "- Python's `glob('filenames.*')` produces an iterable for easy looping\n",
    "- Arithmetic operations on DataFrames results in a **union** of rows across common indices; i.e. if a row exists in A and not in B, the resulting df C will contain a `NaN` value (by default) for the extra row in A.\n",
    "  - Corresponding methods exist in addition to the usual operators.\n",
    "  - E.g.:  `A.add(B)` and `A + B` are equivalent.\n",
    "  - `fill_value=0` param can be used to not fill `NaN`s\n",
    "- *Note: a `NaN` in an arithmetic operation results in a `NaN`.*\n",
    "\n",
    "## Chapter 2: Concatenating data\n",
    "[Slides](slides/Merging DataFrames with pandas/ch2_slides.pdf)\n",
    "\n",
    "### Key methods:\n",
    "- **`.append()`** - DataFrame & Series method to stack *vertically*.\n",
    "- **`pd.concat([df1, df2])`** - Module function to concat vertically or horizontally.\n",
    "  - **`axis=`** option specifies row-wise (0) or column-wise (1) stacking.\n",
    "    - Row-wise does a *UNION* by default\n",
    "    - Column-wise does an *OUTER JOIN* by default\n",
    "  - **`keys=`** option to create a multi-index by input files (on either axis)\n",
    "  - A dict of DFs can also be passed to specify keys to be used in index\n",
    "  - **`join=`** to specify 'inner', 'outer', etc.\n",
    "- `.reset_index()` is often needed after these operations.  `pd.concat(..., ignore_index=True)` is equivalent.\n",
    "\n",
    "### Slicing notes:\n",
    "- A **slicer** `pd.IndexSlice` is required when slicing on an ***inner level** of a MultiIndex*\n",
    "  - Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">bronze</th>\n",
       "      <th>United States</th>\n",
       "      <td>1052.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>475.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>454.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">silver</th>\n",
       "      <th>United States</th>\n",
       "      <td>1195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>627.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>France</th>\n",
       "      <td>461.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">gold</th>\n",
       "      <th>United States</th>\n",
       "      <td>2088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soviet Union</th>\n",
       "      <td>838.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Italy</th>\n",
       "      <td>460.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Germany</th>\n",
       "      <td>407.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Total\n",
       "       Country               \n",
       "bronze United States   1052.0\n",
       "       Soviet Union     584.0\n",
       "       United Kingdom   505.0\n",
       "       France           475.0\n",
       "       Germany          454.0\n",
       "silver United States   1195.0\n",
       "       Soviet Union     627.0\n",
       "       United Kingdom   591.0\n",
       "       France           461.0\n",
       "       Italy            394.0\n",
       "gold   United States   2088.0\n",
       "       Soviet Union     838.0\n",
       "       United Kingdom   498.0\n",
       "       Italy            460.0\n",
       "       Germany          407.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compose DataFrame\n",
    "bronze = pd.DataFrame({'Total': [1052.0, 584.0, 505.0, 475.0, 454.0]})\n",
    "bronze.index = pd.Index(['United States', 'Soviet Union', 'United Kingdom', 'France', 'Germany'],\n",
    "                        name='Country')\n",
    "silver = pd.DataFrame({'Total': [1195.0, 627.0, 591.0, 461.0, 394.0]})\n",
    "silver.index = pd.Index(['United States', 'Soviet Union', 'United Kingdom', 'France', 'Italy'],\n",
    "                        name='Country')\n",
    "gold = pd.DataFrame({'Total': [2088.0, 838.0, 498.0, 460.0, 407.0]})\n",
    "gold.index = pd.Index(['United States', 'Soviet Union', 'United Kingdom', 'Italy', 'Germany'],\n",
    "                        name='Country')\n",
    "medals = pd.concat([bronze, silver, gold], keys=['bronze', 'silver', 'gold'])\n",
    "medals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bronze</th>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gold</th>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>498.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>silver</th>\n",
       "      <th>United Kingdom</th>\n",
       "      <td>591.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Total\n",
       "       Country              \n",
       "bronze United Kingdom  505.0\n",
       "gold   United Kingdom  498.0\n",
       "silver United Kingdom  591.0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Slice on inner index\n",
    "# NOTE: **Index must be sorted first!!!\n",
    "medals_sorted = medals.sort_index()\n",
    "medals_sorted.loc[pd.IndexSlice[:, 'United Kingdom'], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Merging data\n",
    "[Slides](slides/Merging DataFrames with pandas/ch3_slides.pdf)\n",
    "\n",
    "- **`pd.merge(A, B)`** - Merges two DataFrames across common row or column labels.  Does an **INNER join** on all columns with matching names by default.\n",
    "  - `on=`\n",
    "  - `right_on=`\n",
    "  - `left_on=`\n",
    "  - `how='inner'` - The default.  Other values: 'left', 'right', 'outer'\n",
    "  \n",
    "\n",
    "- **`.join()`** - DataFrame method to join another DF.  On index labels by default.\n",
    "  - `how='left'` - The default.  Other values: 'right', 'inner', 'outer'\n",
    "  \n",
    "\n",
    "- **`pd.merge_ordered(A, B)`** - Merges then sorts.  Does an **OUTER join** by default (vs. `.merge()`'s inner).\n",
    "  - `fill_method='ffill'` is handy option\n",
    "- **`pd.merge_asof(A, B)`** - Similar to `.merge_ordered()` but for each row in the left DataFrame A only rows from the right DataFrame B whose 'on' column values are less than the left value will be kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Case Study - Summer Olympics\n",
    "[Slides](slides/Merging DataFrames with pandas/ch4_slides.pdf)\n",
    "\n",
    "- **expanding mean** -- the value of the mean with all the data available up to that point in time.\n",
    "  - `.expanding()` -- A Series and DataFrame method to provide expanding transformations.  Chain with `.expanding().mean()`.\n",
    "- **`.pct_change()`** -- A Series and DataFrame method to calculate percent change from previous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
