{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course: Deep Learning in Python\n",
    "- [DataCamp course link](https://www.datacamp.com/courses/deep-learning-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pre-load modules used later\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Basics of deep learning and neural networks\n",
    "- [Slides](slides/ch1_slides.pdf)\n",
    "\n",
    "\n",
    "- The key difference in neural networks vs. classic regression models is can capture the *interactions* between features which also affect targets.  Regression just captures features' *direct* outcome on the target.\n",
    "- More nodes = more interactions which can be captured.\n",
    "- *representation learning* -- They build representations of patterns in the data which are useful for making predictions.  Patterns increase in complexity in successive layers of the network.  This partially replaces the need for feature engineering.\n",
    "\n",
    "**Forward propogation**\n",
    "- Uses dot product of matrices representing node *values* & *weights*.\n",
    "\n",
    "\n",
    "**Activation functions**\n",
    "- Current best practice & most widely-used is the **ReLU (Rectified Linear Unit)** activation function. Variations will be discussed later (i.e. leaky ReLU, softmax).\n",
    ">*\n",
    "\\begin{equation}\n",
    "ReLU(x) =\n",
    "\\begin{cases}\n",
    "0, & \\text{if  $x$ < 0}\\\\\n",
    "x, & \\text{if  $x$ >= 0}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    ">*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/10/17160725/relu.png\" width=\"450\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# ReLU function visualized.  \n",
    "# Reference: https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/\n",
    "#\n",
    "Image(url=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/10/17160725/relu.png\", width=450)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 -1 -1 5 0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Exercise: Manually calculate values of nodes in a 3-layer network (without an activation function)\n",
    "#\n",
    "from numpy import array\n",
    "\n",
    "# Hidden layer 1\n",
    "n1 = (array([1, 1]) * array([2, 4])).sum()\n",
    "n2 = (array([1, 1]) * array([4, -5])).sum()\n",
    "# Hidden layer 2\n",
    "n3 = (array([n1, n2]) * array([0, 1])).sum()\n",
    "n4 = (array([n1, n2]) * array([1, 1])).sum()\n",
    "# Output layer\n",
    "output = (array([n3, n4]) * array([5, 1])).sum()\n",
    "\n",
    "print(n1, n2, n3, n4, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Optimizing a neural network with backpropagation\n",
    "- [Slides](slides/ch2_slides.pdf)\n",
    "\n",
    "\n",
    "- **Loss function** -- A measure of a model's predictive performance, it aggregates prediction errors from many data points/nodes into single value. (E.g. mean squared error (MSE))\n",
    "- **Gradient descent** is used to find the best (lowest) value of the loss function.\n",
    "    - FYI: Mathematically, an array of slopes is called a \"gradient.\"\n",
    "- **Learning rate**\n",
    "    - Weights are updated by subtracting **```learning rate * slope```**\n",
    "    - Btw calculating it involves the good ol' *chain rule* from calculus! ;) \n",
    "- **Backpropagation**\n",
    "    - Basic steps:\n",
    "        1. Start with random set of weights\n",
    "        1. Apply forward propagation\n",
    "        1. Apply backward propagation to calculate slope of loss function w.r.t. each weight\n",
    "        1. Update weights by multiplying those slopes by the learning rate and subtracting it from the previous weights\n",
    "        1. Iterate until slope of loss function levels out\n",
    "    - Gradients for weight is *the product of*:\n",
    "        1. Node value *feeding into* the weight\n",
    "        1. Slope of loss function w.r.t the node value it *feeds into* (aka prediction error)\n",
    "        1. Slope of *activation function* for the node it feeds into (when using ReLU: 0 if < 0, 1 if >= 0)\n",
    " - **Stochastic gradient descent** done in small batches of input data is often used for computational efficiency.\n",
    "     - *stochastic* = randomly determined.\n",
    "     - Each batch is an *epoch*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Building deep learning models with keras\n",
    "- [Slides](slides/ch3_slides.pdf)\n",
    "\n",
    "\n",
    "[**Keras**](keras.io) is a high-level library capable of running on top of TensorFlow, Microsoft Cognitive Toolkit, Theano, or PlaidML.\n",
    "\n",
    "\n",
    "### Model building steps:\n",
    "1. Specify network architecture\n",
    "1. Compile\n",
    "    - Specify optimizer ('[*adam*](https://keras.io/optimizers/#adam)' is usually a good choice)\n",
    "    - Specify loss function (*'mean_squared_error'* is common for regression, *‘categorical_crossentropy’* aka logloss for classification)\n",
    "    - For classification problems:\n",
    "        - Add `metrics=['accuracy']` for better explainability\n",
    "        - Output layer\n",
    "            1. has node for each possible classification outcome and\n",
    "            1. uses *'softmax'* activation to ensure predictions sum to 1 so they can be used as probabilities.\n",
    "1. Fit\n",
    "    - Applies foward and back propagation & gradient descent\n",
    "1. Predict\n",
    "\n",
    "Model types:\n",
    "- `keras.models.Sequential` -- Layers are connected only in a sequential fashion.\n",
    "\n",
    "Layer types:\n",
    "- `keras.layers.Dense` -- All nodes of a layer connect to all nodes of the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4: Fine-tuning keras models\n",
    "- [Slides](slides/ch4_slides.pdf)\n",
    "\n",
    "\n",
    "More optimizers:\n",
    "- **Stachastic gradient descent (SGD)** -- `keras.optimizers.SGD`\n",
    "\n",
    "\n",
    "Common optimization problems:\n",
    "- **dead neurons** -- Happens when node values get stuck at zero, as with ReLU activation.\n",
    "- **vanishing gradient** -- Happens when many layers have very small slopes (e.g due to being on flat part of tanh activation curve).  Leads to backprop weight updates close to zero.\n",
    "\n",
    "Model training optimization (`model.fit()`):\n",
    "- **Validation**\n",
    "    - One **validation split** data set is commonly used in deep learning instead of k-fold cross-validation as with classical models, due to the large size of data sets used.  The implications are:\n",
    "        - Cross-validation on large data sets is too compute intensive\n",
    "        - The reliability of a larger validation data set is better\n",
    "    - `validation_split=` param of `fit()`.\n",
    "- **Early stopping** -- Quit iterating when model performance stops improving.  \n",
    "    - `keras.callbacks.EarlyStopping`\n",
    "    - Specify using the `callbacks=` param of `fit()`.\n",
    "    \n",
    "**Model or network capacity** -- Similar to overfitting/underfitting (where model complexity is the factor you're trying to optimize).  Too complex of a model can overfit the data used to train it.\n",
    "- Workflow for optimizing model capacity:\n",
    "    1. Start w/ small network\n",
    "    1. Get validation score\n",
    "    1. Keep increasing capacity till validation score no longer improves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
